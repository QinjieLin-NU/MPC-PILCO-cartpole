{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_utils import get_envs\n",
    "import numpy as np\n",
    "from env_utils import initialize_envs, close_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {}\n",
    "arg_dict[\"env\"] = \"CartpoleSwingup\"\n",
    "arg_dict[\"dim_in\"] = 6\n",
    "arg_dict[\"dim_out\"] = 4\n",
    "arg_dict[\"dim_states\"] = 4\n",
    "arg_dict[\"dim_actions\"] = 1\n",
    "arg_dict[\"dim_angles\"] = 1\n",
    "arg_dict[\"target_reward\"] = -0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_dict = {}\n",
    "arg_dict[\"env\"] = \"PendulumEnv\"\n",
    "arg_dict[\"dim_in\"] = 4\n",
    "arg_dict[\"dim_out\"] = 2\n",
    "arg_dict[\"dim_states\"] = 2\n",
    "arg_dict[\"dim_actions\"] = 1\n",
    "arg_dict[\"dim_angles\"] = 1\n",
    "arg_dict[\"target_reward\"] = -0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_envs, test_envs = get_envs(**arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "envs = initialize_envs(training_envs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = envs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"action space high: \", env1.action_space.high)\n",
    "print(\"action space low: \", env1.action_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def anylize_env(env, test_episodes = 200,max_episode_step = 1000, render = False):\n",
    "    print(\"state space shape: \", env.observation_space.shape)\n",
    "    print(\"state space lower bound: \", env.observation_space.low)\n",
    "    print(\"state space upper bound: \", env.observation_space.high)\n",
    "    print(\"action space shape: \", env.action_space.shape)\n",
    "    print(\"action space lower bound: \", env.action_space.low)\n",
    "    print(\"action space upper bound: \", env.action_space.high)\n",
    "    print(\"reward range: \", env.reward_range)\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for episode in range(test_episodes):\n",
    "        env.reset()\n",
    "        step = 0\n",
    "        episode_reward = 0\n",
    "        for _ in range(max_episode_step):\n",
    "            if render:\n",
    "                env.render()\n",
    "            step += 1\n",
    "            action = env.action_space.sample()\n",
    "            state, reward,done,_= env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "               # print(\"done with step: %s \" % (step))\n",
    "                break\n",
    "        steps.append(step)\n",
    "        rewards.append(episode_reward)\n",
    "    env.close()\n",
    "    print(\"Randomly sample actions for %s episodes, with maximum %s steps per episodes\"\n",
    "          % (test_episodes, max_episode_step))\n",
    "    print(\" average reward per episode: %s, std: %s \" % (np.mean(rewards), np.std(rewards) ))\n",
    "    print(\" average steps per episode: \", np.mean(steps))\n",
    "    print(\" average reward per step: \", np.sum(rewards)/np.sum(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.nn import init\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterized_truncated_normal(uniform, mu, sigma, a, b):\n",
    "    normal = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "    alpha = (a - mu) / sigma\n",
    "    beta = (b - mu) / sigma\n",
    "\n",
    "    alpha_normal_cdf = normal.cdf(alpha)\n",
    "    p = alpha_normal_cdf + (normal.cdf(beta) - alpha_normal_cdf) * uniform\n",
    "\n",
    "    p = p.numpy()\n",
    "    one = np.array(1, dtype=p.dtype)\n",
    "    epsilon = np.array(np.finfo(p.dtype).eps, dtype=p.dtype)\n",
    "    v = np.clip(2 * p - 1, -one + epsilon, one - epsilon)\n",
    "    x = mu + sigma * np.sqrt(2) * torch.erfinv(torch.from_numpy(v))\n",
    "    x = torch.clamp(x, a[0], b[0])\n",
    "    return x\n",
    "\n",
    "def sample_truncated_normal(shape=(), mu=0.0, sigma=1.0, a=-2, b=2):\n",
    "    #uni = torch.from_numpy(np.random.uniform(0, 1, shape))\n",
    "    uni = torch.rand(shape)\n",
    "    return parameterized_truncated_normal(uni, mu=mu, sigma=sigma, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mean = np.array([0,1,2,3])\n",
    "init_var = np.array([1,4,2,3])\n",
    "lb = -4\n",
    "ub = 8\n",
    "popsize = 100000\n",
    "sol_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "mean, var = init_mean, init_var\n",
    "a, b = torch.tensor([lb]*sol_dim), torch.tensor([ub]*sol_dim)\n",
    "size = [popsize,sol_dim]\n",
    "lb_dist, ub_dist = mean - lb, ub - mean\n",
    "constrained_var = np.minimum(np.minimum(np.square(lb_dist / 2), np.square(ub_dist / 2)), var)\n",
    "#constrained_var = np.sqrt(constrained_var)\n",
    "mu= torch.tensor(mean)\n",
    "sigma = torch.tensor(constrained_var)\n",
    "r1 = sample_truncated_normal(size, mu, sigma, a, b).numpy()\n",
    "print(time.time()-t)\n",
    "\n",
    "fig, axs = plt.subplots(sol_dim,sharex=True)\n",
    "for i in range(2):\n",
    "    axs[i].hist(r1[:, i], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "\n",
    "r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var = init_mean, init_var\n",
    "t = time.time()\n",
    "X = stats.truncnorm(lb, ub, loc=np.zeros_like(mean), scale=np.ones_like(mean))\n",
    "lb_dist, ub_dist = mean - lb, ub - mean\n",
    "constrained_var = np.minimum(np.minimum(np.square(lb_dist / 2), np.square(ub_dist / 2)), var)\n",
    "r = X.rvs(size=[popsize, sol_dim]) * np.sqrt(constrained_var) + mean\n",
    "print(time.time()-t)\n",
    "\n",
    "fig, axs = plt.subplots(sol_dim,sharex=True)\n",
    "for i in range(2):\n",
    "    axs[i].hist(r[:, i], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "\n",
    "r.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,sharex=True)\n",
    "\n",
    "axs[0].hist(r1[:, 0], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "axs[1].hist(r[:, 0], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "axs[2].hist(r1[:, 1], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "axs[3].hist(r[:, 1], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = X.rvs(size=[popsize, sol_dim]) * np.sqrt(constrained_var) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (t < self.max_iters) and np.max(var) > self.epsilon:\n",
    "    lb_dist, ub_dist = mean - self.lb, self.ub - mean\n",
    "    constrained_var = np.minimum(np.minimum(np.square(lb_dist / 2), np.square(ub_dist / 2)), var)\n",
    "\n",
    "    samples = X.rvs(size=[self.popsize, self.sol_dim]) * np.sqrt(constrained_var) + mean\n",
    "    costs = self.cost_function(samples)\n",
    "    elites = samples[np.argsort(costs)][:self.num_elites]\n",
    "\n",
    "    new_mean = np.mean(elites, axis=0)\n",
    "    new_var = np.var(elites, axis=0)\n",
    "\n",
    "    mean = self.alpha * mean + (1 - self.alpha) * new_mean\n",
    "    var = self.alpha * var + (1 - self.alpha) * new_var\n",
    "\n",
    "    t += 1\n",
    "sol, solvar = mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = np.array([-0.5,0.5])\n",
    "var = np.array([1,1])\n",
    "lb, ub = -1, 1\n",
    "a, b = (lb-init_mean)/init_var, (ub-init_mean)/init_var \n",
    "popsize, sol_dim = 100000, 2\n",
    "X = stats.truncnorm(a, b, loc=init_mean, scale=init_var)\n",
    "lb_dist, ub_dist = mean - lb, ub - mean\n",
    "constrained_var = np.minimum(np.minimum(np.square(lb_dist / 2), np.square(ub_dist / 2)), var)\n",
    "\n",
    "t = time.time()\n",
    "r = X.rvs(size=[popsize, sol_dim]) \n",
    "print(time.time()-t)\n",
    "plt.hist(r[:,1], density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpc_pendulum import MPC\n",
    "from utils import *\n",
    "from models.dynamic_model import DynamicModel\n",
    "from tqdm import trange, tqdm\n",
    "import copy\n",
    "\n",
    "#training_envs = ['PendulumEnv_070-070-v0']\n",
    "training_env_params = [\n",
    "            (0.7, 0.7),\n",
    "            (0.9, 0.9),\n",
    "            (0.7, 0.9),\n",
    "            (0.9, 0.7)\n",
    "        ]\n",
    "\n",
    "test_env_params = [\n",
    "            (0.8, 0.8),\n",
    "            (1.0, 1.0),\n",
    "            (0.8, 1.0),\n",
    "            (1.0, 0.8)\n",
    "        ]\n",
    "\n",
    "training_envs, test_envs = get_env_names(\n",
    "    \"PendulumEnv\", training_env_params, test_env_params)\n",
    "\n",
    "seed = 1\n",
    "envs_train = initialize_envs(training_envs, seed)\n",
    "envs_test = initialize_envs(test_envs, seed)\n",
    "\n",
    "env = envs_train[0]\n",
    "\n",
    "\n",
    "config_path = \"config-new.yml\"\n",
    "config = load_config(config_path)\n",
    "nn_config = config['NN_config']\n",
    "mpc_config = config['mpc_config']\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "mpc_controller.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "model = DynamicModel(NN_config=nn_config)\n",
    "\n",
    "\n",
    "pretrain_episodes = 50\n",
    "max_step = 100\n",
    "#data_list = []\n",
    "for epi in range(pretrain_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    for i in range(max_step):\n",
    "        if done:\n",
    "            break\n",
    "        action = env.action_space.sample()\n",
    "        obs_next, reward, done, state_next = env.step(action)\n",
    "        data = [0, obs, action, obs_next - obs]\n",
    "        #data_list.append(data)\n",
    "        model.add_data_point(data)\n",
    "        obs = copy.deepcopy(obs_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode = 2\n",
    "test_epoch = 20\n",
    "for ep in range(test_epoch):\n",
    "    print('epoch: ', ep)\n",
    "\n",
    "    for epi in range(test_episode):\n",
    "        #print('episode: ', epi)\n",
    "        acc_reward = 0\n",
    "        obs = env.reset()\n",
    "\n",
    "        done = False\n",
    "        mpc_controller.reset()\n",
    "        for i in range(max_step):\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "\n",
    "            action = np.array([mpc_controller.act(model=model, state=obs)])\n",
    "            obs_next, reward, done, state_next = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "            # append data but not training\n",
    "            model.add_data_point([0, obs, action, obs_next - obs])\n",
    "            obs = copy.deepcopy(obs_next)\n",
    "            acc_reward += reward\n",
    "            # logger.info('reward: {}', reward)\n",
    "            #time.sleep(0.1)\n",
    "        print('step: ', i, 'acc_reward: ', acc_reward)\n",
    "        env.close()\n",
    "\n",
    "    # use the collected date to train model\n",
    "    print('fitting the model...')\n",
    "    #model.n_epochs = 20\n",
    "    model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
